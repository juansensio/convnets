{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[VGG](https://arxiv.org/abs/1409.1556) won the Imagenet competition in 2014 and it introduced some key concepts that are still used today. The main idea is to use a stack of convolutional layers with small filters (3x3) that don't change the spatial resolution of the input feature maps, followed by max pooling layers wich reduce the dimensionality by half. Also, every time the dimensions are reduced, the number of filters in the next layers are doubled. \n",
    "\n",
    "![vgg](pics/vgg.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Sequential as S \n",
    "from torch.nn import Conv2d as C\n",
    "from torch.nn import ReLU as R \n",
    "from torch.nn import MaxPool2d as M\n",
    "\n",
    "vgg11config = [\n",
    "    {'repeat': 1, 'filters': 64},\n",
    "    {'repeat': 1, 'filters': 128},\n",
    "    {'repeat': 2, 'filters': 256},\n",
    "    {'repeat': 2, 'filters': 512},\n",
    "    {'repeat': 2, 'filters': 512},\n",
    "]\n",
    "\n",
    "def vgg_block(r, f1, f2):\n",
    "    return S(*[S(C(f1 if i == 0 else f2,f2,3,1,1),R()) for i in range(r)],M(2,2))\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, conf):\n",
    "        super(VGG, self).__init__()\n",
    "        self.backbone = S(*[vgg_block(r['repeat'], conf[i-1]['filters'] if i > 0 else 3, r['filters']) for i, r in enumerate(conf)])\n",
    "        self.head = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(output_size=(7, 7)), # makes it work with any input size (useful for testing at 256x256)\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(512*7*7, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(4096, 1000),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.head(self.backbone(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1000])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(1, 3, 224, 224)\n",
    "model = VGG(vgg11config)\n",
    "model(x).shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the hyperparameters used in the original paper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='4' class='' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      40.00% [4/10 03:44&lt;05:36 loss 6.90765 t1err 0.99885 t5err 0.99441 val_loss 6.90767 val_t1err 1.00000 val_t5err 0.99426]\n",
       "    </div>\n",
       "    \n",
       "Epoch 1/10 loss 6.90783 t1err 0.99912 t5err 0.99517 val_loss 6.90823 val_t1err 1.00000 val_t5err 0.99617<p>Epoch 2/10 loss 6.90792 t1err 0.99889 t5err 0.99533 val_loss 6.90818 val_t1err 1.00000 val_t5err 0.99617<p>Epoch 3/10 loss 6.90781 t1err 0.99920 t5err 0.99514 val_loss 6.90769 val_t1err 0.99809 val_t5err 0.99426<p>Epoch 4/10 loss 6.90765 t1err 0.99885 t5err 0.99441 val_loss 6.90767 val_t1err 1.00000 val_t5err 0.99426<p>\n",
       "\n",
       "    <div>\n",
       "      <progress value='63' class='' max='5005' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      1.26% [63/5005 00:22&lt;29:32 loss 6.90760 t1err 0.99901 t5err 0.99516]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 28\u001b[0m\n\u001b[1;32m     25\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m     26\u001b[0m metrics \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39mt1err\u001b[39m\u001b[39m'\u001b[39m: error, \u001b[39m'\u001b[39m\u001b[39mt5err\u001b[39m\u001b[39m'\u001b[39m: top5_error}\n\u001b[0;32m---> 28\u001b[0m hist \u001b[39m=\u001b[39m fit(\n\u001b[1;32m     29\u001b[0m     model, \n\u001b[1;32m     30\u001b[0m     dataloaders, \n\u001b[1;32m     31\u001b[0m     optimizer, \n\u001b[1;32m     32\u001b[0m     criterion,\n\u001b[1;32m     33\u001b[0m     metrics, \n\u001b[1;32m     34\u001b[0m     max_epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, \u001b[39m# original paper says 74 epochs \u001b[39;49;00m\n\u001b[1;32m     35\u001b[0m     device\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mcuda\u001b[39;49m\u001b[39m'\u001b[39;49m, \n\u001b[1;32m     36\u001b[0m     after_val\u001b[39m=\u001b[39;49m\u001b[39mlambda\u001b[39;49;00m val_logs: scheduler\u001b[39m.\u001b[39;49mstep(val_logs[\u001b[39m'\u001b[39;49m\u001b[39mt1err\u001b[39;49m\u001b[39m'\u001b[39;49m][\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]), \n\u001b[1;32m     37\u001b[0m     limit_train_batches\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, \u001b[39m# comment to train on full dataset\u001b[39;49;00m\n\u001b[1;32m     38\u001b[0m     limit_val_batches\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m \u001b[39m# comment to validate on full dataset\u001b[39;49;00m\n\u001b[1;32m     39\u001b[0m )\n",
      "File \u001b[0;32m~/Desktop/convnets/models/convnets/train/fit.py:69\u001b[0m, in \u001b[0;36mfit\u001b[0;34m(model, dataloader, optimizer, criterion, metrics, max_epochs, overfit_batches, limit_train_batches, limit_val_batches, after_epoch_log, on_epoch_end, after_val, device, rank, compile, *args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m     68\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m---> 69\u001b[0m train_logs[\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mappend(loss\u001b[39m.\u001b[39;49mitem())\n\u001b[1;32m     70\u001b[0m \u001b[39mfor\u001b[39;00m metric \u001b[39min\u001b[39;00m metrics\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m     71\u001b[0m     train_logs[metric]\u001b[39m.\u001b[39mappend(metrics[metric](y_hat, y)\u001b[39m.\u001b[39mitem())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from convnets.datasets import ImageNet\n",
    "import albumentations as A\n",
    "from convnets.train import seed_everything\n",
    "from torch.utils.data import DataLoader\n",
    "from convnets.train import fit \n",
    "from convnets.metrics import error, top5_error\n",
    "\n",
    "seed_everything()\n",
    "\n",
    "trans = A.Compose([\n",
    "    A.RandomCrop(224, 224),\n",
    "    A.HorizontalFlip(),\n",
    "    A.RGBShift()\n",
    "])\n",
    "\n",
    "path = '/fastdata/imagenet256' \n",
    "dataloaders  =  {\n",
    "    'train': DataLoader(ImageNet(path, 'train', trans), batch_size=256, shuffle=True, num_workers=10, pin_memory=True),\n",
    "    'val': DataLoader(ImageNet(path, 'val'), batch_size=256, shuffle=False, num_workers=10, pin_memory=True),\n",
    "}\n",
    "\n",
    "model = VGG(vgg11config)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9, weight_decay=0.0005)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=1, factor=0.1, verbose=True, threshold_mode='abs', min_lr=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "metrics = {'t1err': error, 't5err': top5_error}\n",
    "\n",
    "hist = fit(\n",
    "    model, \n",
    "    dataloaders, \n",
    "    optimizer, \n",
    "    criterion,\n",
    "    metrics, \n",
    "    max_epochs=10, # original paper says 74 epochs \n",
    "    device='cuda', \n",
    "    after_val=lambda val_logs: scheduler.step(val_logs['t1err'][-1]), \n",
    "    limit_train_batches=100, # comment to train on full dataset\n",
    "    limit_val_batches=100 # comment to validate on full dataset\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['val_t1err', 'val_t5err'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[64], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m fig \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39mfigure(figsize\u001b[39m=\u001b[39m(\u001b[39m7\u001b[39m,\u001b[39m3\u001b[39m))\n\u001b[1;32m      6\u001b[0m ax \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplot(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m df\u001b[39m.\u001b[39;49mplot(x\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mepoch\u001b[39;49m\u001b[39m'\u001b[39;49m, y\u001b[39m=\u001b[39;49m[\u001b[39m'\u001b[39;49m\u001b[39mt1err\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mval_t1err\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mt5err\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mval_t5err\u001b[39;49m\u001b[39m'\u001b[39;49m], ax \u001b[39m=\u001b[39;49m ax)\n\u001b[1;32m      8\u001b[0m ax\u001b[39m.\u001b[39mgrid(\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m      9\u001b[0m ax \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplot(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m2\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/plotting/_core.py:986\u001b[0m, in \u001b[0;36mPlotAccessor.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    983\u001b[0m             \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[39m# don't overwrite\u001b[39;00m\n\u001b[0;32m--> 986\u001b[0m data \u001b[39m=\u001b[39m data[y]\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m    988\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ABCSeries):\n\u001b[1;32m    989\u001b[0m     label_name \u001b[39m=\u001b[39m label_kw \u001b[39mor\u001b[39;00m y\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:3811\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3809\u001b[0m     \u001b[39mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3810\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[0;32m-> 3811\u001b[0m     indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49m_get_indexer_strict(key, \u001b[39m\"\u001b[39;49m\u001b[39mcolumns\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m1\u001b[39m]\n\u001b[1;32m   3813\u001b[0m \u001b[39m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(indexer, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39mbool\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:6113\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6110\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   6111\u001b[0m     keyarr, indexer, new_indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6113\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[1;32m   6115\u001b[0m keyarr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtake(indexer)\n\u001b[1;32m   6116\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6117\u001b[0m     \u001b[39m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:6176\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6173\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNone of [\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m] are in the [\u001b[39m\u001b[39m{\u001b[39;00maxis_name\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   6175\u001b[0m not_found \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m]]\u001b[39m.\u001b[39munique())\n\u001b[0;32m-> 6176\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mnot_found\u001b[39m}\u001b[39;00m\u001b[39m not in index\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['val_t1err', 'val_t5err'] not in index\""
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.DataFrame(hist)\n",
    "fig = plt.figure(figsize=(7,3))\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "df.plot(x='epoch', y=['t1err', 'val_t1err', 't5err', 'val_t5err'], ax = ax)\n",
    "ax.grid(True)\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "df.plot(x='epoch', y=['loss', 'val_loss'], ax = ax)\n",
    "ax.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Originally, it took about 2-3 weekas on four NVIDIA Titan Black GPUs to train the model. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
